{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIRECTORY = \"./dataset/yelp2018\"\n",
    "MAX_INTERACTIONS = 100_000\n",
    "MAX_USERS = 6000\n",
    "MAX_ITEMS = 20000\n",
    "NEGATIVE_SAMPLING_RATIO = 6\n",
    "\n",
    "SEED = 6969\n",
    "TRAIN_SIZE = 0.9\n",
    "VALIDATION_SIZE = 0.1\n",
    "BATCH_SIZE = 1024\n",
    "NUM_LATENT_FACTOR = 8\n",
    "LEARNING_RATE = 0.001\n",
    "WEIGHT_DACAY = 0.00001\n",
    "PROJECT_NAME = f'RecSys-NeuMF-Yelp2018-Latent{NUM_LATENT_FACTOR}-NegRatio{NEGATIVE_SAMPLING_RATIO}-with-Metrics'\n",
    "\n",
    "import random\n",
    "import pandas as pd\n",
    "import wandb\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from metrics import MetronAtK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NeuMF Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NCFModel(pl.LightningModule):\n",
    "    def __init__(self, num_users, num_items, latent_factors, num_mlp_layers=[16, 64, 32, 16, 8], top_k=10):\n",
    "        super().__init__()\n",
    "        self.num_users = num_users\n",
    "        self.num_items = num_items\n",
    "        self.top_k = top_k\n",
    "        \n",
    "        # GMF\n",
    "        ## GMF USER EMBEDDING\n",
    "        self.gmf_user_embedding = nn.Embedding(num_embeddings=num_users, embedding_dim=latent_factors)\n",
    "        nn.init.normal_(self.gmf_user_embedding.weight, mean=0, std=0.01)\n",
    "        self.gmf_user_embedding.weight = nn.Parameter(self.gmf_user_embedding.weight, requires_grad=True)\n",
    "        ## GMF ITEM EMBEDDING\n",
    "        self.gmf_item_embedding = nn.Embedding(num_embeddings=num_items, embedding_dim=latent_factors)\n",
    "        nn.init.normal_(self.gmf_item_embedding.weight, mean=0, std=0.01)\n",
    "        self.gmf_item_embedding.weight = nn.Parameter(self.gmf_item_embedding.weight, requires_grad=True)\n",
    "        \n",
    "        # MLP\n",
    "        ## MLP USER EMBEDDING\n",
    "        self.mlp_user_embedding = nn.Embedding(num_embeddings=num_users, embedding_dim=latent_factors)\n",
    "        ## MLP ITEM EMBEDDING\n",
    "        self.mlp_item_embedding = nn.Embedding(num_embeddings=num_items, embedding_dim=latent_factors)\n",
    "        ## MLP LAYERS\n",
    "        mlp_layers = []\n",
    "        input_dim = num_mlp_layers[0]  # Concatenated embedding dimension\n",
    "        for idx in range(1, len(num_mlp_layers)):\n",
    "            mlp_layers.append(\n",
    "                nn.Linear(input_dim, num_mlp_layers[idx])\n",
    "            )\n",
    "            mlp_layers.append(nn.ReLU())\n",
    "            input_dim = num_mlp_layers[idx]\n",
    "        self.mlp = nn.Sequential(*mlp_layers)\n",
    "            \n",
    "        # NeuMF\n",
    "        self.neumf_layer = nn.Linear(in_features=num_mlp_layers[-1] + latent_factors, out_features=1)\n",
    "        self.neumf_sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        # Weight Initialize\n",
    "        for sm in self.modules():\n",
    "            if isinstance(sm, (nn.Embedding, nn.Linear)):\n",
    "                nn.init.normal_(sm.weight.data, 0.0, 0.01)\n",
    "                \n",
    "    def forward(self, user_input, item_input):\n",
    "        # GMF\n",
    "        ## GMF EMBEDDING\n",
    "        gmf_user_embedding = self.gmf_user_embedding(user_input)\n",
    "        gmf_item_embedding = self.gmf_item_embedding(item_input)\n",
    "        ## GMF ELEMENT-WISE PRODUCT\n",
    "        gmf_element_wise_product = torch.mul(gmf_user_embedding, gmf_item_embedding)\n",
    "        \n",
    "        # MLP\n",
    "        ## MLP EMBEDDING\n",
    "        mlp_user_embedding = self.mlp_user_embedding(user_input)\n",
    "        mlp_item_embedding = self.mlp_item_embedding(item_input)\n",
    "        mlp_concat_embedding = torch.cat([mlp_user_embedding, mlp_item_embedding], dim=-1)\n",
    "\n",
    "        ## MLP FORWARD\n",
    "        mlp_vector = mlp_concat_embedding\n",
    "        for idx, _ in enumerate(range(len(self.mlp))):\n",
    "            mlp_vector = self.mlp[idx](mlp_vector)\n",
    "            \n",
    "        # NeuMF\n",
    "        ## CONCAT GMF & MLP\n",
    "        neumf_concat_embedding = torch.cat([gmf_element_wise_product, mlp_vector], dim=-1)\n",
    "        prediction = self.neumf_sigmoid(self.neumf_layer(neumf_concat_embedding))\n",
    "        \n",
    "        return prediction\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DACAY)\n",
    "        return optimizer\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        user_input, item_input, labels = batch\n",
    "        predictions = self.forward(user_input, item_input).squeeze()\n",
    "        loss = nn.BCELoss()(predictions, labels.float())\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def ndcg_at_k(self, predictions, labels, k):\n",
    "        \"\"\"\n",
    "        Compute NDCG@K metric.\n",
    "        Args:\n",
    "            predictions (torch.Tensor): Predicted scores.\n",
    "            labels (torch.Tensor): Ground truth labels (binary).\n",
    "            k (int): Top K items to consider.\n",
    "        Returns:\n",
    "            float: NDCG@K value.\n",
    "        \"\"\"\n",
    "        _, indices = torch.topk(predictions, k, largest=True, sorted=True)\n",
    "        top_k_labels = labels[indices]\n",
    "        dcg = torch.sum((2 ** top_k_labels - 1) / torch.log2(torch.arange(2, k + 2).float()))\n",
    "        ideal_dcg = torch.sum((2 ** torch.sort(labels, descending=True).values[:k] - 1) / torch.log2(torch.arange(2, k + 2).float()))\n",
    "        return (dcg / ideal_dcg) if ideal_dcg > 0 else 0.0\n",
    "\n",
    "\n",
    "    def hit_ratio_at_k(self, predictions, labels, k):\n",
    "        \"\"\"\n",
    "        Compute HR@K metric.\n",
    "        Args:\n",
    "            predictions (torch.Tensor): Predicted scores.\n",
    "            labels (torch.Tensor): Ground truth labels (binary).\n",
    "            k (int): Top K items to consider.\n",
    "        Returns:\n",
    "            float: HR@K value.\n",
    "        \"\"\"\n",
    "        _, indices = torch.topk(predictions, k, largest=True, sorted=True)\n",
    "        top_k_labels = labels[indices]\n",
    "        return torch.sum(top_k_labels).item() > 0\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        user_input, item_input, labels = batch\n",
    "        predictions = self.forward(user_input, item_input).squeeze()\n",
    "        loss = nn.BCELoss()(predictions, labels.float())\n",
    "        \n",
    "        # Calculate metrics using self.top_k\n",
    "        ndcg = self.ndcg_at_k(predictions, labels, self.top_k)\n",
    "        hr = self.hit_ratio_at_k(predictions, labels, self.top_k)\n",
    "        \n",
    "        # Log metrics\n",
    "        self.log(\"val_loss\", loss, prog_bar=True)\n",
    "        self.log(\"val_ndcg\", ndcg, prog_bar=True)\n",
    "        self.log(\"val_hr\", hr, prog_bar=True)\n",
    "        \n",
    "        return {\"loss\": loss, \"ndcg\": ndcg, \"hr\": hr}\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        user_input, item_input, labels = batch\n",
    "        predictions = self.forward(user_input, item_input).squeeze()\n",
    "        loss = nn.BCELoss()(predictions, labels.float())\n",
    "        self.log(\"test_loss\", loss)\n",
    "        return loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        return sample[0], sample[1], sample[2]\n",
    "\n",
    "class Yelp2018DataModule(pl.LightningDataModule):\n",
    "    def __init__(self, data_dir=DATA_DIRECTORY, batch_size=BATCH_SIZE):\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    def process_data(self, file_path):\n",
    "        with open(file_path, 'r') as file:\n",
    "            data = file.readlines()\n",
    "            \n",
    "            user_item_dict = {}\n",
    "            interaction_count = 0\n",
    "            original_user_ids = []\n",
    "            original_item_ids = set()\n",
    "            \n",
    "            for user_id in range(MAX_USERS):\n",
    "                tokens = list(map(int, data[user_id].strip().split()))\n",
    "                item_ids = tokens[1:]\n",
    "                if len(item_ids) < 20 or max(item_ids) > MAX_ITEMS:\n",
    "                    continue\n",
    "                else:\n",
    "                    original_user_ids.append(user_id)\n",
    "                    user_item_dict[user_id] = item_ids\n",
    "                    interaction_count += len(item_ids)\n",
    "                    original_item_ids.update(item_ids)\n",
    "            \n",
    "            # Re-index user and item IDs\n",
    "            user_id_map = {original_id: new_id for new_id, original_id in enumerate(original_user_ids)}\n",
    "            item_id_map = {original_id: new_id for new_id, original_id in enumerate(sorted(original_item_ids))}\n",
    "            \n",
    "            self.num_users = len(user_id_map)\n",
    "            self.num_items = len(item_id_map)\n",
    "            print(f'Read data from {file_path}')\n",
    "            print(f'Filtered {self.num_users:,d} unique users & {self.num_items:,d} unique items.')\n",
    "            print(f'Interaction count: {interaction_count:,d} records.\\n')\n",
    "            \n",
    "            data_processed = []\n",
    "            for original_user_id, item_ids in user_item_dict.items():\n",
    "                reindexed_user_id = user_id_map[original_user_id]\n",
    "                reindexed_item_ids = [item_id_map[item_id] for item_id in item_ids]\n",
    "                # POSITIVE\n",
    "                for reindexed_item_id in reindexed_item_ids:\n",
    "                    data_processed.append([reindexed_user_id, reindexed_item_id, 1])\n",
    "                # NEGATIVE\n",
    "                all_items = set(range(len(item_id_map)))\n",
    "                non_interacted_items = all_items - set(reindexed_item_ids)\n",
    "                negative_samples = random.sample(list(non_interacted_items), len(item_ids) * NEGATIVE_SAMPLING_RATIO)\n",
    "                for reindexed_item_id in negative_samples:\n",
    "                    data_processed.append([reindexed_user_id, reindexed_item_id, 0])\n",
    "            return data_processed\n",
    "    \n",
    "    def setup(self, stage=None):\n",
    "        self.test_data = self.process_data(f\"{self.data_dir}/test.txt\")\n",
    "        full_data = self.process_data(f\"{self.data_dir}/train.txt\")\n",
    "        self.train_data, self.val_data = random_split(full_data, [TRAIN_SIZE, VALIDATION_SIZE])\n",
    "        \n",
    "        self.train_dataset = CustomDataset(self.train_data)\n",
    "        self.val_dataset = CustomDataset(self.val_data)\n",
    "        self.test_dataset = CustomDataset(self.test_data)\n",
    "          \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size=self.batch_size)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_dataset, batch_size=self.batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_module = Yelp2018DataModule()\n",
    "# data_module.setup()\n",
    "\n",
    "# train_loader = data_module.train_dataloader()\n",
    "# for batch in train_loader:\n",
    "#     print(\"Batch sample:\", batch)\n",
    "#     break  # Only view the first batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read data from ./dataset/yelp2018/test.txt\n",
      "Filtered 48 unique users & 1,031 unique items.\n",
      "Interaction count: 1,223 records.\n",
      "\n",
      "Read data from ./dataset/yelp2018/train.txt\n",
      "Filtered 1,970 unique users & 19,643 unique items.\n",
      "Interaction count: 102,391 records.\n",
      "\n",
      "                                                                           \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\PytorchLightning_ENV\\Lib\\site-packages\\wandb\\sdk\\wandb_run.py:2374: UserWarning: Run (7ih8o773) is finished. The call to `_console_raw_callback` will be ignored. Please make sure that you are using an active run.\n",
      "  lambda data: self._console_raw_callback(\"stdout\", data),\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:8gt095md) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">peach-terrain-27</strong> at: <a href='https://wandb.ai/suttawee-pom-chulalongkorn-university/uncategorized/runs/8gt095md' target=\"_blank\">https://wandb.ai/suttawee-pom-chulalongkorn-university/uncategorized/runs/8gt095md</a><br/> View project at: <a href='https://wandb.ai/suttawee-pom-chulalongkorn-university/uncategorized' target=\"_blank\">https://wandb.ai/suttawee-pom-chulalongkorn-university/uncategorized</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20241121_115132-8gt095md\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Fatal error while uploading data. Some run data will not be synced, but it will still be written to disk. Use `wandb sync` at the end of the run to try uploading."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:8gt095md). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\Suttawee\\Desktop\\ncf\\wandb\\run-20241121_115220-4vi9lpap</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/suttawee-pom-chulalongkorn-university/uncategorized/runs/4vi9lpap' target=\"_blank\">sunny-pine-28</a></strong> to <a href='https://wandb.ai/suttawee-pom-chulalongkorn-university/uncategorized' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/suttawee-pom-chulalongkorn-university/uncategorized' target=\"_blank\">https://wandb.ai/suttawee-pom-chulalongkorn-university/uncategorized</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/suttawee-pom-chulalongkorn-university/uncategorized/runs/4vi9lpap' target=\"_blank\">https://wandb.ai/suttawee-pom-chulalongkorn-university/uncategorized/runs/4vi9lpap</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\PytorchLightning_ENV\\Lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read data from ./dataset/yelp2018/test.txt\n",
      "Filtered 48 unique users & 1,031 unique items.\n",
      "Interaction count: 1,223 records.\n",
      "\n",
      "Read data from ./dataset/yelp2018/train.txt\n",
      "Filtered 1,970 unique users & 19,643 unique items.\n",
      "Interaction count: 102,391 records.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name               | Type       | Params | Mode \n",
      "----------------------------------------------------------\n",
      "0 | gmf_user_embedding | Embedding  | 15.8 K | train\n",
      "1 | gmf_item_embedding | Embedding  | 157 K  | train\n",
      "2 | mlp_user_embedding | Embedding  | 15.8 K | train\n",
      "3 | mlp_item_embedding | Embedding  | 157 K  | train\n",
      "4 | mlp                | Sequential | 3.8 K  | train\n",
      "5 | neumf_layer        | Linear     | 17     | train\n",
      "6 | neumf_sigmoid      | Sigmoid    | 0      | train\n",
      "----------------------------------------------------------\n",
      "349 K     Trainable params\n",
      "0         Non-trainable params\n",
      "349 K     Total params\n",
      "1.399     Total estimated model params size (MB)\n",
      "15        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                            "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\PytorchLightning_ENV\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "c:\\ProgramData\\anaconda3\\envs\\PytorchLightning_ENV\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5:  77%|███████▋  | 482/630 [00:28<00:08, 16.93it/s, v_num=lpap, val_loss=0.319, val_ndcg=0.872, val_hr=1.000]"
     ]
    }
   ],
   "source": [
    "# Callbacks\n",
    "checkpoint_callback = ModelCheckpoint(monitor=\"val_loss\")\n",
    "#early_stopping_callback = EarlyStopping(monitor=\"val_loss\", patience=5, mode=\"min\")\n",
    "\n",
    "# Data and Model\n",
    "datamodule = Yelp2018DataModule()\n",
    "datamodule.setup()\n",
    "model = NCFModel(num_users=datamodule.num_users, num_items=datamodule.num_items, latent_factors=NUM_LATENT_FACTOR)\n",
    "# print(model)\n",
    "\n",
    "# # Trainer\n",
    "# logger = TensorBoardLogger(\"logs\", name=\"ncf\")\n",
    "\n",
    "# initialise the wandb logger and name your wandb project\n",
    "wandb_logger = WandbLogger(project=PROJECT_NAME)\n",
    "wandb.init()\n",
    "# add your batch size to the wandb config\n",
    "wandb_logger.experiment.config[\"batch_size\"] = BATCH_SIZE\n",
    "trainer = Trainer(\n",
    "    fast_dev_run=False,\n",
    "    logger=wandb_logger,\n",
    "    max_epochs=200,\n",
    "    callbacks=[checkpoint_callback],\n",
    "    log_every_n_steps=50,\n",
    ")\n",
    "trainer.fit(model, datamodule=datamodule)\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PytorchLightning_ENV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
